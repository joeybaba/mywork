{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  category                                 theme  \\\n0       汽车  新辉腾　４．２　Ｖ８　４座加长Ｉｎｄｉｖｉｄｕａｌ版２０１１款　最新报价   \n1       汽车                         ９１８　Ｓｐｙｄｅｒ概念车   \n2       汽车              日内瓦亮相　ＭＩＮＩ性能版／概念车－１．６Ｔ引擎   \n3       汽车                清仓大甩卖一汽夏利Ｎ５威志Ｖ２低至３．３９万   \n4       汽车                    大众敞篷家族新成员　高尔夫敞篷版实拍   \n\n                                                 URL  \\\n0        http://auto.data.people.com.cn/model_15782/   \n1  http://auto.data.people.com.cn/prdview_165423....   \n2  http://auto.data.people.com.cn/news/story_5249...   \n3  http://auto.data.people.com.cn/news/story_6144...   \n4  http://auto.data.people.com.cn/news/story_5686...   \n\n                                             content  \n0  经销商　电话　试驾／订车Ｕ憬杭州滨江区江陵路１７８０号４００８－１１２２３３转５８６４＃保常...  \n1       呼叫热线　４００８－１００－３００　服务邮箱　ｋｆ＠ｐｅｏｐｌｅｄａｉｌｙ．ｃｏｍ．ｃｎ  \n2  ＭＩＮＩ品牌在二月曾经公布了最新的ＭＩＮＩ新概念车Ｃｌｕｂｖａｎ效果图，不过现在在日内瓦车展...  \n3  清仓大甩卖！一汽夏利Ｎ５、威志Ｖ２低至３．３９万＝日，启新中国一汽强势推出一汽夏利Ｎ５、威志...  \n4  在今年３月的日内瓦车展上，我们见到了高尔夫家族的新成员，高尔夫敞篷版，这款全新敞篷车受到了众...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>category</th>\n      <th>theme</th>\n      <th>URL</th>\n      <th>content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>汽车</td>\n      <td>新辉腾　４．２　Ｖ８　４座加长Ｉｎｄｉｖｉｄｕａｌ版２０１１款　最新报价</td>\n      <td>http://auto.data.people.com.cn/model_15782/</td>\n      <td>经销商　电话　试驾／订车Ｕ憬杭州滨江区江陵路１７８０号４００８－１１２２３３转５８６４＃保常...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>汽车</td>\n      <td>９１８　Ｓｐｙｄｅｒ概念车</td>\n      <td>http://auto.data.people.com.cn/prdview_165423....</td>\n      <td>呼叫热线　４００８－１００－３００　服务邮箱　ｋｆ＠ｐｅｏｐｌｅｄａｉｌｙ．ｃｏｍ．ｃｎ</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>汽车</td>\n      <td>日内瓦亮相　ＭＩＮＩ性能版／概念车－１．６Ｔ引擎</td>\n      <td>http://auto.data.people.com.cn/news/story_5249...</td>\n      <td>ＭＩＮＩ品牌在二月曾经公布了最新的ＭＩＮＩ新概念车Ｃｌｕｂｖａｎ效果图，不过现在在日内瓦车展...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>汽车</td>\n      <td>清仓大甩卖一汽夏利Ｎ５威志Ｖ２低至３．３９万</td>\n      <td>http://auto.data.people.com.cn/news/story_6144...</td>\n      <td>清仓大甩卖！一汽夏利Ｎ５、威志Ｖ２低至３．３９万＝日，启新中国一汽强势推出一汽夏利Ｎ５、威志...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>汽车</td>\n      <td>大众敞篷家族新成员　高尔夫敞篷版实拍</td>\n      <td>http://auto.data.people.com.cn/news/story_5686...</td>\n      <td>在今年３月的日内瓦车展上，我们见到了高尔夫家族的新成员，高尔夫敞篷版，这款全新敞篷车受到了众...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "### 数据源：http://www.sogou.com/labs/resource/ca.php ###\n",
    "file = '/Users/joey/Documents/PycharmProjects/mywork/bayesian_algorithm/val.txt'\n",
    "df_news = pd.read_table(file, names=['category', 'theme', 'URL', 'content'], encoding= 'utf-8')\n",
    "df_news = df_news.dropna()\n",
    "df_news.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(5000, 4)"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "df_news.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['经销商\\u3000电话\\u3000试驾／订车Ｕ憬杭州滨江区江陵路１７８０号４００８－１１２２３３转５８６４＃保常叮００万９阒菔邪自魄白云大道北１３６１号；广州市天河区黄埔大道西１００号富力盈泰大厦１０５室４００８－１１２２３３转９９１５＃保常福００万Ｉ蕉省淄博市张店区山泉路８９号４００８－１１２２３３转５１５６＃保常叮００万４罅保税区黄海西三路１０１号４００８－１１２２３３转２６０３＃保玻埃００万Ｌ粕绞新纺锨复兴路２１号４００８－１１２２３３转３０４３＃保常叮００万Ｖ泄云南昆明市度假区滇池路１２６８号４００８－１１２２３３转７３１２＃保常叮００万Ｒ川市兴庆区丽景北街８００号４００８－１１２２３３转３２６９＃保常叮００万９尔滨市道外区先锋路４６９号４００８－１１２２３３转２０２９＃保矗福００万３ど呈刑煨那桂花坪街道雀园路口／星沙中南汽车世界Ａ区０５号４００８－１１２２３３转７６６６＃保常梗００万Ｎ浜菏信塘城经济开发区盘龙汽车城＃矗埃埃福１１２２３３转７５２４＃保常叮００万９阒莘禺区市广路９８９号（祈福食街旁）＃矗埃埃福１１２２３３转９９６３＃保常叮００万Ｆ侄新区御桥路１３７７号４００８－１１２２３３转６３３７＃保常福００万０不帐『戏适邪河工业区纬一路２２号１３８．００万Ｉ虾Ｊ斜ι角江杨南路１３８１号４００８－１１２２３３转６７２２＃保常叮００万ｔ奚蕉路１９８号４００８－１１２２３３转５９３３＃保常叮００万１本┦谐阳区北四环望京街６８号４００８－１１２２３３转８６１５＃保玻福００万１本┦胁平区立汤路亚北博晟汽车汇展中心＃保埃福８６万＝西省南昌市青山湖区科技大道５９９号１３６．００万Ｉ苄耸信劢工业区康宁路车管所对面＃保常叮００万Ｄ暇┦薪宁区天元中路１１１号４００８－１１２２３３转５５０１＃保常叮００万３ご菏形餍戮济技术开发区长沈路４２２２号１３６．００万Ｊ家庄市北二环东路８６号河北国际汽车贸易园区＃矗埃埃福１１２２３３转３１７８＃保矗福００万８壅⑶城港路９９号广达车城永兴路３号１３６．００万Ｉ蜓羰刑西区北二中路１１号４００８－１１２２３３转２４９８＃保常叮００万３啥际星嘌虼蟮溃保福负牛ㄐ挛幕宫对面）＃保矗常８０万Ａ赡省沈阳市皇姑区鸭绿江街３２号甲（长客总站北行１５００米）＃保矗福００万Ｉ钲谑新藓区罗芳立交六星汽车园进口大众４Ｓ店４００８－１１２２３３转９８６６＃保担埃００万３ご憾环城路１００５６号１３６．００万', '呼叫热线\\u3000４００８－１００－３００\\u3000服务邮箱\\u3000ｋｆ＠ｐｅｏｐｌｅｄａｉｌｙ．ｃｏｍ．ｃｎ']\n"
    }
   ],
   "source": [
    "# 分词\n",
    "# tolist() 数组或矩阵转为列表\n",
    "content = df_news.content.values.tolist()\n",
    "print(content[0: 2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Building prefix dict from the default dictionary ...\nDumping model to file cache /var/folders/ky/dw312wn14611cjbt05d227dr0000gn/T/jieba.cache\nLoading model cost 0.920 seconds.\nPrefix dict has been built successfully.\n"
    }
   ],
   "source": [
    "content_S = []\n",
    "for line in content:\n",
    "    current_segment = jieba.lcut(line)\n",
    "    if len(current_segment) > 1 and current_segment != '\\r\\n':\n",
    "        content_S.append(current_segment)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[['经销商', '\\u3000', '电话', '\\u3000', '试驾', '／', '订车', 'Ｕ', '憬', '杭州', '滨江区', '江陵', '路', '１', '７', '８', '０', '号', '４', '０', '０', '８', '－', '１', '１', '２', '２', '３', '３', '转', '５', '８', '６', '４', '＃', '保常', '叮', '０', '０', '万', '９', '阒', '菔', '邪', '自魄', '白云', '大道北', '１', '３', '６', '１', '号', '；', '广州市', '天河区', '黄埔', '大道', '西', '１', '０', '０', '号', '富力', '盈泰', '大厦', '１', '０', '５', '室', '４', '０', '０', '８', '－', '１', '１', '２', '２', '３', '３', '转', '９', '９', '１', '５', '＃', '保常福', '０', '０', '万', 'Ｉ', '蕉', '省', '淄博市', '张店区', '山泉', '路', '８', '９', '号', '４', '０', '０', '８', '－', '１', '１', '２', '２', '３', '３', '转', '５', '１', '５', '６', '＃', '保常', '叮', '０', '０', '万', '４', '罅', '保税区', '黄海', '西', '三路', '１', '０', '１', '号', '４', '０', '０', '８', '－', '１', '１', '２', '２', '３', '３', '转', '２', '６', '０', '３', '＃', '保玻埃', '０', '０', '万', 'Ｌ', '粕', '绞', '新', '纺锨', '复兴路', '２', '１', '号', '４', '０', '０', '８', '－', '１', '１', '２', '２', '３', '３', '转', '３', '０', '４', '３', '＃', '保常', '叮', '０', '０', '万', 'Ｖ', '泄', '云南', '昆明市', '度假区', '滇池', '路', '１', '２', '６', '８', '号', '４', '０', '０', '８', '－', '１', '１', '２', '２', '３', '３', '转', '７', '３', '１', '２', '＃', '保常', '叮', '０', '０', '万', 'Ｒ', '川市', '兴庆区', '丽景', '北街', '８', '０', '０', '号', '４', '０', '０', '８', '－', '１', '１', '２', '２', '３', '３', '转', '３', '２', '６', '９', '＃', '保常', '叮', '０', '０', '万', '９', '尔滨市', '道外区', '先锋', '路', '４', '６', '９', '号', '４', '０', '０', '８', '－', '１', '１', '２', '２', '３', '３', '转', '２', '０', '２', '９', '＃', '保', '矗福', '０', '０', '万', '３', 'ど', '呈刑', '煨', '那', '桂花', '坪', '街道', '雀园', '路口', '／', '星沙', '中南', '汽车', '世界', 'Ａ', '区', '０', '５', '号', '４', '０', '０', '８', '－', '１', '１', '２', '２', '３', '３', '转', '７', '６', '６', '６', '＃', '保常', '梗', '０', '０', '万', 'Ｎ', '浜', '菏', '信塘城', '经济', '开发区', '盘龙', '汽车城', '＃', '矗埃埃福', '１', '１', '２', '２', '３', '３', '转', '７', '５', '２', '４', '＃', '保常', '叮', '０', '０', '万', '９', '阒莘禺', '区市', '广路', '９', '８', '９', '号', '（', '祈福', '食街', '旁', '）', '＃', '矗埃埃福', '１', '１', '２', '２', '３', '３', '转', '９', '９', '６', '３', '＃', '保常', '叮', '０', '０', '万', 'Ｆ', '侄', '新区', '御桥', '路', '１', '３', '７', '７', '号', '４', '０', '０', '８', '－', '１', '１', '２', '２', '３', '３', '转', '６', '３', '３', '７', '＃', '保常福', '０', '０', '万', '０', '不帐', '『', '戏适', '邪河', '工业区', '纬', '一路', '２', '２', '号', '１', '３', '８', '．', '０', '０', '万', 'Ｉ', '虾', 'Ｊ', '斜', 'ι', '角江', '杨', '南路', '１', '３', '８', '１', '号', '４', '０', '０', '８', '－', '１', '１', '２', '２', '３', '３', '转', '６', '７', '２', '２', '＃', '保常', '叮', '０', '０', '万', 'ｔ', '奚蕉路', '１', '９', '８', '号', '４', '０', '０', '８', '－', '１', '１', '２', '２', '３', '３', '转', '５', '９', '３', '３', '＃', '保常', '叮', '０', '０', '万', '１', '本', '┦', '谐阳区', '北四环', '望京', '街', '６', '８', '号', '４', '０', '０', '８', '－', '１', '１', '２', '２', '３', '３', '转', '８', '６', '１', '５', '＃', '保玻福', '０', '０', '万', '１', '本', '┦', '胁', '平区立', '汤路', '亚北博晟', '汽车', '汇展', '中心', '＃', '保埃福', '８', '６', '万', '＝', '西省', '南昌市', '青山湖区', '科技', '大道', '５', '９', '９', '号', '１', '３', '６', '．', '０', '０', '万', 'Ｉ', '苄', '耸信', '劢', '工业区', '康宁', '路', '车管所', '对面', '＃', '保常', '叮', '０', '０', '万', 'Ｄ', '暇', '┦', '薪宁区', '天元', '中路', '１', '１', '１', '号', '４', '０', '０', '８', '－', '１', '１', '２', '２', '３', '３', '转', '５', '５', '０', '１', '＃', '保常', '叮', '０', '０', '万', '３', 'ご', '菏形', '餍', '戮济', '技术开发区', '长沈路', '４', '２', '２', '２', '号', '１', '３', '６', '．', '０', '０', '万', 'Ｊ', '家庄', '市', '北二环', '东路', '８', '６', '号', '河北', '国际', '汽车贸易', '园区', '＃', '矗埃埃福', '１', '１', '２', '２', '３', '３', '转', '３', '１', '７', '８', '＃', '保', '矗福', '０', '０', '万', '８', '壅', '⑶', '城港路', '９', '９', '号', '广达', '车城', '永兴路', '３', '号', '１', '３', '６', '．', '０', '０', '万', 'Ｉ', '蜓', '羰刑', '西区', '北二', '中路', '１', '１', '号', '４', '０', '０', '８', '－', '１', '１', '２', '２', '３', '３', '转', '２', '４', '９', '８', '＃', '保常', '叮', '０', '０', '万', '３', '啥际星', '嘌', '虼', '蟮', '溃', '保福', '负牛', 'ㄐ', '挛幕宫', '对面', '）', '＃', '保', '矗常', '８', '０', '万', 'Ａ', '赡省', '沈阳市', '皇姑区', '鸭绿江', '街', '３', '２', '号', '甲', '（', '长客', '总站', '北', '行', '１', '５', '０', '０', '米', '）', '＃', '保', '矗福', '０', '０', '万', 'Ｉ', '钲', '谑', '新藓区', '罗芳', '立交', '六星', '汽车', '园', '进口', '大众', '４', 'Ｓ', '店', '４', '０', '０', '８', '－', '１', '１', '２', '２', '３', '３', '转', '９', '８', '６', '６', '＃', '保担', '埃', '０', '０', '万', '３', 'ご', '憾', '环城路', '１', '０', '０', '５', '６', '号', '１', '３', '６', '．', '０', '０', '万'], ['呼叫', '热线', '\\u3000', '４', '０', '０', '８', '－', '１', '０', '０', '－', '３', '０', '０', '\\u3000', '服务', '邮箱', '\\u3000', 'ｋ', 'ｆ', '＠', 'ｐ', 'ｅ', 'ｏ', 'ｐ', 'ｌ', 'ｅ', 'ｄ', 'ａ', 'ｉ', 'ｌ', 'ｙ', '．', 'ｃ', 'ｏ', 'ｍ', '．', 'ｃ', 'ｎ']]\n"
    }
   ],
   "source": [
    "print(content_S[0: 2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                           content_S\n0  [经销商, 　, 电话, 　, 试驾, ／, 订车, Ｕ, 憬, 杭州, 滨江区, 江陵, ...\n1  [呼叫, 热线, 　, ４, ０, ０, ８, －, １, ０, ０, －, ３, ０, ０...\n2  [Ｍ, Ｉ, Ｎ, Ｉ, 品牌, 在, 二月, 曾经, 公布, 了, 最新, 的, Ｍ, Ｉ...\n3  [清仓, 大, 甩卖, ！, 一汽, 夏利, Ｎ, ５, 、, 威志, Ｖ, ２, 低至, ...\n4  [在, 今年, ３, 月, 的, 日内瓦, 车展, 上, ，, 我们, 见到, 了, 高尔夫...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>content_S</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[经销商, 　, 电话, 　, 试驾, ／, 订车, Ｕ, 憬, 杭州, 滨江区, 江陵, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[呼叫, 热线, 　, ４, ０, ０, ８, －, １, ０, ０, －, ３, ０, ０...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[Ｍ, Ｉ, Ｎ, Ｉ, 品牌, 在, 二月, 曾经, 公布, 了, 最新, 的, Ｍ, Ｉ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[清仓, 大, 甩卖, ！, 一汽, 夏利, Ｎ, ５, 、, 威志, Ｖ, ２, 低至, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[在, 今年, ３, 月, 的, 日内瓦, 车展, 上, ，, 我们, 见到, 了, 高尔夫...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "df_content = pd.DataFrame({'content_S': content_S})\n",
    "df_content.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  stopword\n0        !\n1        \"\n2        #\n3        $\n4        %",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>stopword</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>!</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>\"</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>#</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>$</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>%</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "file1 = '/Users/joey/Documents/PycharmProjects/mywork/bayesian_algorithm/stopwords.txt'\n",
    "stopwords = pd.read_csv(file1, index_col=False, sep='\\t', quoting=3, names=['stopword'], encoding='utf-8')\n",
    "stopwords.head()\n",
    "\n",
    "# quoting : int or csv.QUOTE_* instance, default 0\n",
    "#     Control field quoting behavior per ``csv.QUOTE_*`` constants. Use one of\n",
    "#     QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).\n",
    "# 当文本文件中带有英文双引号时，直接用pd.read_csv进行读取会导致行数减少，\n",
    "# 此时应该对read_csv设置参数quoting=3或者quoting=csv.QUOTE_NONE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def drop_stopwords(contents, stopwords):\n",
    "    contents_clean = []\n",
    "    all_words = []\n",
    "    for line in contents:\n",
    "        line_clean = []\n",
    "        for word in line:\n",
    "            if word in stopwords:\n",
    "                continue\n",
    "            line_clean.append(word)\n",
    "            all_words.append(str(word))\n",
    "        contents_clean.append(line_clean)\n",
    "    return contents_clean, all_words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "raw_contents = df_content.content_S.values.tolist()\n",
    "raw_stopwords = stopwords.stopword.values.tolist()\n",
    "contents_clean, all_words = drop_stopwords(raw_contents,raw_stopwords)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "993928"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "len(all_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                      contents_clean\n0  [经销商, 电话, 试驾, 订车, Ｕ, 憬, 杭州, 滨江区, 江陵, 路, 号, 转, ...\n1  [呼叫, 热线, 服务, 邮箱, ｋ, ｆ, ｐ, ｅ, ｏ, ｐ, ｌ, ｅ, ｄ, ａ,...\n2  [Ｍ, Ｉ, Ｎ, Ｉ, 品牌, 二月, 公布, 最新, Ｍ, Ｉ, Ｎ, Ｉ, 新, 概念...\n3  [清仓, 甩卖, 一汽, 夏利, Ｎ, 威志, Ｖ, 低至, 万, 启新, 中国, 一汽, ...\n4  [日内瓦, 车展, 见到, 高尔夫, 家族, 新, 成员, 高尔夫, 敞篷版, 款, 全新,...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>contents_clean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[经销商, 电话, 试驾, 订车, Ｕ, 憬, 杭州, 滨江区, 江陵, 路, 号, 转, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[呼叫, 热线, 服务, 邮箱, ｋ, ｆ, ｐ, ｅ, ｏ, ｐ, ｌ, ｅ, ｄ, ａ,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[Ｍ, Ｉ, Ｎ, Ｉ, 品牌, 二月, 公布, 最新, Ｍ, Ｉ, Ｎ, Ｉ, 新, 概念...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[清仓, 甩卖, 一汽, 夏利, Ｎ, 威志, Ｖ, 低至, 万, 启新, 中国, 一汽, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[日内瓦, 车展, 见到, 高尔夫, 家族, 新, 成员, 高尔夫, 敞篷版, 款, 全新,...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "df_content = pd.DataFrame({'contents_clean': contents_clean})\n",
    "df_content.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  all_words\n0       经销商\n1        电话\n2        试驾\n3        订车\n4         Ｕ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>all_words</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>经销商</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>电话</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>试驾</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>订车</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Ｕ</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "df_all_words = pd.DataFrame({'all_words': all_words})\n",
    "df_all_words.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  all_words  count\n0       经销商      1\n1        电话      1\n2        试驾      1\n3        订车      1\n4         Ｕ      1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>all_words</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>经销商</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>电话</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>试驾</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>订车</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Ｕ</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "# words_count = df_all_words.groupby('all_words').agg({'counts': 'np.size'})\n",
    "# SpecificationError: nested renamer is not supported\n",
    "\n",
    "df_all_words.loc[:,'count'] = 1\n",
    "df_all_words.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       all_words  count\n4077           中   5199\n4209          中国   3115\n88255          说   3055\n104747         Ｓ   2646\n1373           万   2390",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>all_words</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4077</th>\n      <td>中</td>\n      <td>5199</td>\n    </tr>\n    <tr>\n      <th>4209</th>\n      <td>中国</td>\n      <td>3115</td>\n    </tr>\n    <tr>\n      <th>88255</th>\n      <td>说</td>\n      <td>3055</td>\n    </tr>\n    <tr>\n      <th>104747</th>\n      <td>Ｓ</td>\n      <td>2646</td>\n    </tr>\n    <tr>\n      <th>1373</th>\n      <td>万</td>\n      <td>2390</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "words_count = df_all_words.groupby('all_words').agg({'count': 'count'})\n",
    "# words_count = df_all_words.groupby('all_words').agg({'all_words': 'count'})\n",
    "words_count = words_count.reset_index().sort_values(by=['count'], ascending=False)\n",
    "words_count.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 5.0)\n",
    "font = '/Users/joey/Library/Fonts/TT-Me.ttf'\n",
    "wordcloud = WordCloud(font_path=font, background_color='white', max_font_size=80)\n",
    "word_frequence = {x[0]: x[1] for x in words_count.head(100).values}\n",
    "wordcloud = wordcloud.fit_words(word_frequence)\n",
    "# plt.imshow(wordcloud)\n",
    "# wordcloud.generate_from_frequencies(word_frequence)\n",
    "# wordcloud.to_file('a.png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TF-IDF : 提取关键词"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "法国ＶＳ西班牙、里贝里ＶＳ哈维，北京时间６月２４日凌晨一场的大战举世瞩目，而这场胜利不仅仅关乎两支顶级强队的命运，同时也是他们背后的球衣赞助商耐克和阿迪达斯之间的一次角逐。Ｔ谌胙”窘炫分薇的１６支球队之中，阿迪达斯和耐克的势力范围也是几乎旗鼓相当：其中有５家球衣由耐克提供，而阿迪达斯则赞助了６家，此外茵宝有３家，而剩下的两家则由彪马赞助。而当比赛进行到现在，率先挺进四强的两支球队分别被耐克支持的葡萄牙和阿迪达斯支持的德国占据，而由于最后一场１／４决赛是茵宝（英格兰）和彪马（意大利）的对决，这也意味着明天凌晨西班牙同法国这场阿迪达斯和耐克在１／４决赛的唯一一次直接交手将直接决定两家体育巨头在此次欧洲杯上的胜负。８据评估，在２０１２年足球商品的销售额能总共超过４０亿欧元，而单单是不足一个月的欧洲杯就有高达５亿的销售额，也就是说在欧洲杯期间将有７００万件球衣被抢购一空。根据市场评估，两大巨头阿迪达斯和耐克的市场占有率也是并驾齐驱，其中前者占据３８％，而后者占据３６％。体育权利顾问奥利弗－米歇尔在接受《队报》采访时说：“欧洲杯是耐克通过法国翻身的一个绝佳机会！”Ｃ仔尔接着谈到两大赞助商的经营策略：“竞技体育的成功会燃起球衣购买的热情，不过即便是水平相当，不同国家之间的欧洲杯效应却存在不同。在德国就很出色，大约１／４的德国人通过电视观看了比赛，而在西班牙效果则差很多，由于民族主义高涨的加泰罗尼亚地区只关注巴萨和巴萨的球衣，他们对西班牙国家队根本没什么兴趣。”因此尽管西班牙接连拿下欧洲杯和世界杯，但是阿迪达斯只为西班牙足协支付每年２６００万的赞助费＃相比之下尽管最近两届大赛表现糟糕法国足协将从耐克手中每年可以得到４０００万欧元。米歇尔解释道：“法国创纪录的４０００万欧元赞助费得益于阿迪达斯和耐克竞逐未来１５年欧洲市场的竞争。耐克需要笼络一个大国来打赢这场欧洲大陆的战争，而尽管德国拿到的赞助费并不太高，但是他们却显然牢牢掌握在民族品牌阿迪达斯手中。从长期投资来看，耐克给法国的赞助并不算过高。”\n"
    }
   ],
   "source": [
    "import jieba.analyse\n",
    "index = 2400\n",
    "print(df_news['content'][index])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'法国ＶＳ西班牙、里贝里ＶＳ哈维，北京时间６月２４日凌晨一场的大战举世瞩目，而这场胜利不仅仅关乎两支顶级强队的命运，同时也是他们背后的球衣赞助商耐克和阿迪达斯之间的一次角逐。Ｔ谌胙”窘炫分薇的１６支球队之中，阿迪达斯和耐克的势力范围也是几乎旗鼓相当：其中有５家球衣由耐克提供，而阿迪达斯则赞助了６家，此外茵宝有３家，而剩下的两家则由彪马赞助。而当比赛进行到现在，率先挺进四强的两支球队分别被耐克支持的葡萄牙和阿迪达斯支持的德国占据，而由于最后一场１／４决赛是茵宝（英格兰）和彪马（意大利）的对决，这也意味着明天凌晨西班牙同法国这场阿迪达斯和耐克在１／４决赛的唯一一次直接交手将直接决定两家体育巨头在此次欧洲杯上的胜负。８据评估，在２０１２年足球商品的销售额能总共超过４０亿欧元，而单单是不足一个月的欧洲杯就有高达５亿的销售额，也就是说在欧洲杯期间将有７００万件球衣被抢购一空。根据市场评估，两大巨头阿迪达斯和耐克的市场占有率也是并驾齐驱，其中前者占据３８％，而后者占据３６％。体育权利顾问奥利弗－米歇尔在接受《队报》采访时说：“欧洲杯是耐克通过法国翻身的一个绝佳机会！”Ｃ仔尔接着谈到两大赞助商的经营策略：“竞技体育的成功会燃起球衣购买的热情，不过即便是水平相当，不同国家之间的欧洲杯效应却存在不同。在德国就很出色，大约１／４的德国人通过电视观看了比赛，而在西班牙效果则差很多，由于民族主义高涨的加泰罗尼亚地区只关注巴萨和巴萨的球衣，他们对西班牙国家队根本没什么兴趣。”因此尽管西班牙接连拿下欧洲杯和世界杯，但是阿迪达斯只为西班牙足协支付每年２６００万的赞助费＃相比之下尽管最近两届大赛表现糟糕法国足协将从耐克手中每年可以得到４０００万欧元。米歇尔解释道：“法国创纪录的４０００万欧元赞助费得益于阿迪达斯和耐克竞逐未来１５年欧洲市场的竞争。耐克需要笼络一个大国来打赢这场欧洲大陆的战争，而尽管德国拿到的赞助费并不太高，但是他们却显然牢牢掌握在民族品牌阿迪达斯手中。从长期投资来看，耐克给法国的赞助并不算过高。”'"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "content_S_str = ''.join(content_S[index])\n",
    "content_S_str"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['耐克', '阿迪达斯', '欧洲杯', '球衣', '西班牙']\n"
    }
   ],
   "source": [
    "# 选择特征\n",
    "print(jieba.analyse.extract_tags(content_S_str, topK=5, withWeight=False))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LDA : 主题模型\n",
    "格式要求：list of list形式，分词好的整个语料"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# 自然语言处理\n",
    "from gensim import corpora, models, similarities\n",
    "import gensim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = [['我们','不能','不能','不能'],['这样','走掉'],['我们','不能'],['我们','墨迹','不能']]\n",
    "dictionary = corpora.Dictionary(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[[(0, 3), (1, 1)],\n [(2, 1), (3, 1)],\n [(0, 1), (1, 1)],\n [(0, 1), (1, 1), (4, 1)]]"
     },
     "metadata": {},
     "execution_count": 84
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(sentence) for sentence in dic]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# 做映射，相当于词袋\n",
    "# 文本特征向量转化模块\n",
    "dictionary = corpora.Dictionary(contents_clean)\n",
    "corpus = [dictionary.doc2bow(sentence) for sentence in contents_clean]\n",
    "# corpus 语料库"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "83, 1),\n  (19263, 1),\n  (19700, 1),\n  (20487, 9),\n  (20598, 1),\n  (20697, 3),\n  (20800, 2),\n  (21098, 5),\n  (21234, 1),\n  (21323, 1),\n  (21344, 1),\n  (22251, 1),\n  (23042, 1),\n  (23187, 1),\n  (23344, 2),\n  (23363, 2),\n  (24269, 1),\n  (24623, 1),\n  (24989, 1),\n  (25102, 1),\n  (27620, 2),\n  (27862, 2),\n  (28019, 1),\n  (28020, 1),\n  (28021, 1),\n  (28022, 1),\n  (28023, 1),\n  (28024, 1),\n  (28025, 1),\n  (28026, 1),\n  (28027, 1),\n  (28028, 1),\n  (28029, 1),\n  (28030, 1),\n  (28031, 1),\n  (28032, 1),\n  (28033, 1),\n  (28034, 1),\n  (28035, 1),\n  (28036, 1),\n  (28037, 1),\n  (28038, 1)],\n [(13, 1),\n  (195, 5),\n  (277, 1),\n  (307, 1),\n  (309, 5),\n  (332, 5),\n  (349, 17),\n  (369, 1),\n  (445, 1),\n  (520, 1),\n  (609, 5),\n  (784, 1),\n  (1026, 9),\n  (1044, 1),\n  (1102, 1),\n  (1156, 1),\n  (1396, 1),\n  (1407, 1),\n  (1435, 1),\n  (1645, 1),\n  (1682, 3),\n  (1684, 3),\n  (1831, 1),\n  (1895, 1),\n  (2081, 2),\n  (2178, 2),\n  (2297, 10),\n  (2318, 1),\n  (2399, 1),\n  (2400, 1),\n  (2458, 1),\n  (2465, 1),\n  (2649, 1),\n  (2736, 1),\n  (2801, 1),\n  (2993, 1),\n  (2995, 2),\n  (3005, 6),\n  (3033, 1),\n  (3040, 1),\n  (3187, 1),\n  (3295, 1),\n  (3301, 1),\n  (3328, 1),\n  (3433, 2),\n  (3683, 1),\n  (4081, 3),\n  (5006, 2),\n  (5519, 4),\n  (5947, 1),\n  (6092, 1),\n  (6174, 1),\n  (6802, 4),\n  (7011, 2),\n  (7070, 1),\n  (7077, 1),\n  (7078, 1),\n  (7953, 1),\n  (7978, 2),\n  (8135, 1),\n  (8241, 2),\n  (8464, 10),\n  (9291, 1),\n  (9327, 7),\n  (9743, 6),\n  (9917, 1),\n  (10209, 1),\n  (10928, 3),\n  (11266, 1),\n  (12164, 4),\n  (12168, 1),\n  (12192, 3),\n  (12232, 1),\n  (12244, 2),\n  (12376, 1),\n  (12635, 1),\n  (12683, 4),\n  (12685, 2),\n  (12734, 4),\n  (12775, 1),\n  (12837, 3),\n  (13043, 1),\n  (13355, 1),\n  (13538, 1),\n  (13582, 1),\n  (13998, 3),\n  (14283, 4),\n  (14752, 1),\n  (15106, 1),\n  (15162, 1),\n  (15198, 3),\n  (15322, 3),\n  (15677, 8),\n  (16339, 3),\n  (16512, 1),\n  (16576, 1),\n  (17335, 1),\n  (18184, 1),\n  (18873, 1),\n  (19779, 1),\n  (22689, 1),\n  (23538, 1),\n  (23719, 1),\n  (23796, 1),\n  (25000, 1),\n  (25660, 2),\n  (25739, 1),\n  (27256, 3),\n  (27383, 1),\n  (28039, 2),\n  (28040, 1),\n  (28041, 1),\n  (28042, 1),\n  (28043, 1),\n  (28044, 2),\n  (28045, 1),\n  (28046, 1),\n  (28047, 3),\n  (28048, 1),\n  (28049, 1),\n  (28050, 1),\n  (28051, 5),\n  (28052, 3),\n  (28053, 1),\n  (28054, 2),\n  (28055, 2),\n  (28056, 1)],\n [(70, 1),\n  (196, 1),\n  (558, 1),\n  (908, 1),\n  (917, 1),\n  (1026, 1),\n  (1144, 1),\n  (1284, 1),\n  (1415, 1),\n  (1972, 1),\n  (2507, 1),\n  (2610, 1),\n  (2970, 1),\n  (3328, 1),\n  (4261, 1),\n  (4286, 1),\n  (4388, 2),\n  (4539, 1),\n  (4611, 1),\n  (5178, 1),\n  (5253, 1),\n  (5261, 2),\n  (5262, 1),\n  (5682, 1),\n  (7013, 1),\n  (7175, 1),\n  (10154, 1),\n  (10814, 1),\n  (11885, 2),\n  (12521, 1),\n  (12596, 1),\n  (12613, 2),\n  (15743, 1),\n  (17167, 1),\n  (18128, 1),\n  (19853, 1),\n  (20495, 2),\n  (20870, 1),\n  (20883, 1),\n  (21875, 1),\n  (22771, 1),\n  (23472, 1),\n  (24153, 1),\n  (28057, 1),\n  (28058, 1),\n  (28059, 1),\n  (28060, 1),\n  (28061, 1),\n  (28062, 1),\n  (28063, 1),\n  (28064, 1),\n  (28065, 3),\n  (28066, 1),\n  (28067, 1),\n  (28068, 2),\n  (28069, 1),\n  (28070, 2),\n  (28071, 1),\n  (28072, 1),\n  (28073, 1)],\n [(195, 1),\n  (265, 1),\n  (349, 1),\n  (432, 1),\n  (470, 1),\n  (921, 1),\n  (1127, 1),\n  (1169, 1),\n  (1327, 1),\n  (1516, 1),\n  (2081, 2),\n  (2104, 1),\n  (2297, 1),\n  (2372, 2),\n  (2465, 1),\n  (2831, 1),\n  (2991, 1),\n  (3187, 1),\n  (4659, 1),\n  (4877, 1),\n  (5064, 1),\n  (6136, 1),\n  (6322, 1),\n  (7384, 1),\n  (7943, 1),\n  (9291, 1),\n  (9433, 1),\n  (9805, 1),\n  (10301, 2),\n  (10831, 1),\n  (11839, 1),\n  (11990, 1),\n  (12022, 1),\n  (12163, 1),\n  (12232, 2),\n  (12295, 1),\n  (12783, 1),\n  (13060, 6),\n  (13186, 4),\n  (13200, 3),\n  (13263, 1),\n  (14103, 1),\n  (14352, 1),\n  (15443, 2),\n  (16274, 6),\n  (16278, 1),\n  (16496, 1),\n  (18186, 2),\n  (20013, 1),\n  (26800, 1),\n  (28074, 1),\n  (28075, 1),\n  (28076, 1),\n  (28077, 1),\n  (28078, 1),\n  (28079, 1),\n  (28080, 1)],\n [(4612, 2),\n  (4697, 2),\n  (6121, 1),\n  (7936, 1),\n  (12006, 1),\n  (12241, 1),\n  (12377, 1),\n  (18142, 2),\n  (20258, 1),\n  (26464, 1),\n  (26740, 1),\n  (28081, 1),\n  (28082, 2),\n  (28083, 1),\n  (28084, 1)],\n [(49, 1),\n  (188, 1),\n  (190, 2),\n  (193, 2),\n  (274, 1),\n  (306, 5),\n  (307, 3),\n  (308, 1),\n  (311, 2),\n  (486, 1),\n  (540, 1),\n  (541, 7),\n  (553, 1),\n  (1030, 1),\n  (1130, 1),\n  (1195, 1),\n  (1198, 2),\n  (1563, 1),\n  (1729, 5),\n  (1757, 2),\n  (1891, 2),\n  (1906, 2),\n  (2545, 1),\n  (3242, 1),\n  (3678, 3),\n  (4260, 1),\n  (4712, 2),\n  (4877, 13),\n  (5192, 1),\n  (5203, 3),\n  (5354, 1),\n  (6092, 1),\n  (6218, 1),\n  (6284, 7),\n  (6294, 5),\n  (6388, 1),\n  (6762, 1),\n  (7323, 1),\n  (8033, 6),\n  (8090, 1),\n  (10575, 2),\n  (10672, 1),\n  (10687, 1),\n  (10927, 1),\n  (11192, 1),\n  (12442, 6),\n  (12463, 1),\n  (12505, 1),\n  (12666, 2),\n  (12822, 4),\n  (12823, 1),\n  (13100, 1),\n  (13215, 2),\n  (13217, 2),\n  (13288, 3),\n  (13593, 1),\n  (13919, 1),\n  (13923, 3),\n  (13924, 1),\n  (13931, 1),\n  (13952, 1),\n  (13953, 1),\n  (14031, 1),\n  (14034, 3),\n  (14615, 1),\n  (15388, 1),\n  (15404, 3),\n  (15415, 1),\n  (15447, 1),\n  (15496, 1),\n  (16358, 2),\n  (16618, 1),\n  (16659, 2),\n  (17306, 1),\n  (17644, 2),\n  (17657, 2),\n  (18277, 1),\n  (18282, 1),\n  (18901, 1),\n  (19626, 4),\n  (19631, 2),\n  (28085, 1),\n  (28086, 1),\n  (28087, 1),\n  (28088, 1)],\n [(89, 1),\n  (191, 1),\n  (195, 1),\n  (196, 1),\n  (206, 2),\n  (214, 1),\n  (215, 2),\n  (281, 1),\n  (309, 2),\n  (314, 1),\n  (349, 3),\n  (372, 2),\n  (414, 1),\n  (441, 1),\n  (445, 2),\n  (469, 1),\n  (541, 1),\n  (565, 4),\n  (616, 1),\n  (640, 2),\n  (712, 1),\n  (765, 1),\n  (824, 1),\n  (908, 6),\n  (945, 1),\n  (958, 4),\n  (1015, 1),\n  (1055, 1),\n  (1093, 1),\n  (1130, 3),\n  (1144, 2),\n  (1166, 1),\n  (1182, 1),\n  (1198, 4),\n  (1232, 1),\n  (1297, 1),\n  (1371, 1),\n  (1407, 1),\n  (1415, 1),\n  (1520, 1),\n  (1524, 2),\n  (1660, 1),\n  (1689, 1),\n  (1712, 1),\n  (1730, 1),\n  (1779, 1),\n  (1794, 1),\n  (1801, 4),\n  (1834, 1),\n  (1898, 1),\n  (1993, 1),\n  (2104, 1),\n  (2108, 1),\n  (2248, 1),\n  (2442, 1),\n  (2494, 3),\n  (2526, 1),\n  (2575, 3),\n  (2748, 1),\n  (2786, 1),\n  (3064, 1),\n  (3228, 1),\n  (3241, 1),\n  (3335, 1),\n  (3670, 1),\n  (3783, 1),\n  (3784, 1),\n  (3788, 1),\n  (3823, 1),\n  (4146, 1),\n  (4673, 1),\n  (5030, 1),\n  (5128, 1),\n  (5178, 2),\n  (5192, 1),\n  (5472, 1),\n  (5519, 1),\n  (5916, 1),\n  (6055, 1),\n  (6059, 1),\n  (6297, 1),\n  (7015, 1),\n  (7070, 1),\n  (7093, 2),\n  (7229, 1),\n  (7247, 1),\n  (7268, 2),\n  (7927, 1),\n  (8483, 1),\n  (8484, 2),\n  (9085, 1),\n  (9291, 1),\n  (9693, 1),\n  (10018, 1),\n  (10100, 1),\n  (10142, 1),\n  (10289, 6),\n  (10360, 1),\n  (10482, 3),\n  (10520, 1),\n  (10663, 3),\n  (11981, 1),\n  (12006, 6),\n  (12130, 1),\n  (12163, 1),\n  (12192, 3),\n  (12374, 1),\n  (12475, 1),\n  (12596, 2),\n  (12641, 3),\n  (12840, 1),\n  (12962, 1),\n  (13007, 2),\n  (13060, 9),\n  (13065, 2),\n  (13082, 1),\n  (13487, 1),\n  (13837, 1),\n  (13858, 3),\n  (13955, 3),\n  (14053, 7),\n  (14250, 1),\n  (14319, 1),\n  (14724, 1),\n  (15110, 1),\n  (15134, 1),\n  (15262, 3),\n  (15582, 1),\n  (15883, 1),\n  (16083, 1),\n  (16422, 2),\n  (16561, 1),\n  (16594, 1),\n  (16865, 1),\n  (17104, 1),\n  (18176, 1),\n  (18498, 2),\n  (18502, 1),\n  (20153, 1),\n  (20258, 1),\n  (20357, 2),\n  (20600, 1),\n  (20661, 1),\n  (21182, 1),\n  (22762, 1),\n  (22815, 4),\n  (22952, 1),\n  (23006, 1),\n  (23329, 1),\n  (23658, 1),\n  (24282, 1),\n  (24769, 1),\n  (28089, 1),\n  (28090, 1),\n  (28091, 1),\n  (28092, 1),\n  (28093, 1),\n  (28094, 1),\n  (28095, 3),\n  (28096, 1),\n  (28097, 1),\n  (28098, 1),\n  (28099, 1),\n  (28100, 1),\n  (28101, 1),\n  (28102, 1),\n  (28103, 1),\n  (28104, 2),\n  (28105, 1),\n  (28106, 4),\n  (28107, 1),\n  (28108, 1),\n  (28109, 2),\n  (28110, 1),\n  (28111, 1),\n  (28112, 1),\n  (28113, 1),\n  (28114, 1),\n  (28115, 1),\n  (28116, 1),\n  (28117, 1),\n  (28118, 1),\n  (28119, 1),\n  (28120, 1),\n  (28121, 1),\n  (28122, 1),\n  (28123, 1),\n  (28124, 1),\n  (28125, 1),\n  (28126, 1),\n  (28127, 1),\n  (28128, 1),\n  (28129, 1),\n  (28130, 1),\n  (28131, 1),\n  (28132, 1),\n  (28133, 1),\n  (28134, 3),\n  (28135, 1),\n  (28136, 1)],\n [(167, 1),\n  (190, 1),\n  (193, 1),\n  (222, 1),\n  (309, 1),\n  (323, 2),\n  (351, 1),\n  (543, 1),\n  (556, 1),\n  (725, 1),\n  (754, 1),\n  (867, 1),\n  (906, 1),\n  (949, 3),\n  (961, 1),\n  (972, 1),\n  (1015, 1),\n  (1110, 1),\n  (1119, 1),\n  (1255, 1),\n  (1475, 1),\n  (1529, 1),\n  (1635, 1),\n  (2063, 1),\n  (2220, 1),\n  (2245, 1),\n  (2629, 1),\n  (2934, 1),\n  (2995, 1),\n  (3328, 1),\n  (3351, 1),\n  (3633, 1),\n  (3692, 2),\n  (3695, 2),\n  (3712, 1),\n  (3856, 1),\n  (4307, 1),\n  (4386, 1),\n  (4673, 1),\n  (4715, 1),\n  (5013, 1),\n  (5275, 1),\n  (5425, 2),\n  (5881, 1),\n  (5992, 1),\n  (6442, 1),\n  (6712, 1),\n  (7190, 1),\n  (7192, 1),\n  (7716, 1),\n  (7999, 2),\n  (8106, 1),\n  (8777, 2),\n  (8851, 1),\n  (9881, 1),\n  (10073, 1),\n  (10393, 1),\n  (10566, 1),\n  (11028, 1),\n  (11308, 1),\n  (11356, 1),\n  (11475, 4),\n  (11503, 1),\n  (11532, 1),\n  (12883, 1),\n  (13694, 1),\n  (13948, 1),\n  (14062, 1),\n  (14286, 1),\n  (14295, 2),\n  (14344, 1),\n  (14946, 1),\n  (14947, 1),\n  (14968, 1),\n  (15611, 1),\n  (16140, 1),\n  (17801, 1),\n  (17927, 1),\n  (18240, 1),\n  (18955, 1),\n  (20713, 1),\n  (21689, 1),\n  (21755, 2),\n  (22106, 1),\n  (22728, 2),\n  (23106, 1),\n  (23373, 1),\n  (23624, 1),\n  (25430, 1),\n  (25733, 1),\n  (26364, 1),\n  (26932, 3),\n  (26969, 1),\n  (28137, 1),\n  (28138, 1),\n  (28139, 1),\n  (28140, 1),\n  (28141, 1),\n  (28142, 1),\n  (28143, 1),\n  (28144, 1),\n  (28145, 1),\n  (28146, 1),\n  (28147, 1),\n  (28148, 2),\n  (28149, 1),\n  (28150, 1),\n  (28151, 1),\n  (28152, 1),\n  (28153, 1),\n  (28154, 2),\n  (28155, 1),\n  (28156, 2),\n  (28157, 1),\n  (28158, 1),\n  (28159, 1),\n  (28160, 2),\n  (28161, 1),\n  (28162, 1),\n  (28163, 1),\n  (28164, 1),\n  (28165, 1),\n  (28166, 1),\n  (28167, 1),\n  (28168, 1),\n  (28169, 3),\n  (28170, 2),\n  (28171, 1),\n  (28172, 1),\n  (28173, 1),\n  (28174, 1),\n  (28175, 2),\n  (28176, 1),\n  (28177, 1),\n  (28178, 1),\n  (28179, 1),\n  (28180, 1),\n  (28181, 1),\n  (28182, 1),\n  (28183, 1),\n  (28184, 1),\n  (28185, 1),\n  (28186, 1),\n  (28187, 1),\n  (28188, 1),\n  (28189, 1),\n  (28190, 1)],\n [(191, 1),\n  (416, 1),\n  (470, 1),\n  (563, 5),\n  (921, 1),\n  (1020, 2),\n  (1169, 1),\n  (1174, 2),\n  (1399, 3),\n  (1472, 1),\n  (1682, 2),\n  (1739, 1),\n  (1778, 1),\n  (1849, 1),\n  (2081, 2),\n  (2233, 1),\n  (2465, 1),\n  (2792, 2),\n  (2883, 1),\n  (2896, 1),\n  (2995, 1),\n  (3384, 1),\n  (4081, 2),\n  (4273, 2),\n  (5177, 1),\n  (5203, 3),\n  (5343, 1),\n  (5971, 1),\n  (6222, 1),\n  (6552, 1),\n  (7440, 2),\n  (8033, 5),\n  (8150, 1),\n  (8897, 2),\n  (9517, 1),\n  (10577, 1),\n  (11984, 1),\n  (12048, 1),\n  (12057, 1),\n  (12107, 1),\n  (12239, 1),\n  (12361, 1),\n  (12683, 3),\n  (13087, 2),\n  (13209, 1),\n  (13216, 1),\n  (13217, 1),\n  (13329, 1),\n  (13379, 3),\n  (13567, 1),\n  (14000, 1),\n  (14083, 1),\n  (14103, 1),\n  (15290, 1),\n  (15297, 3),\n  (15304, 1),\n  (15409, 1),\n  (15433, 1),\n  (15448, 1),\n  (15804, 3),\n  (16496, 2),\n  (16893, 2),\n  (17754, 1),\n  (17786, 3),\n  (18652, 4),\n  (20425, 3),\n  (21899, 1),\n  (23230, 1),\n  (24300, 3),\n  (24301, 3),\n  (25183, 1),\n  (28191, 3),\n  (28192, 1)],\n [(19, 2),\n  (154, 1),\n  (193, 1),\n  (197, 1),\n  (200, 2),\n  (307, 1),\n  (309, 1),\n  (310, 4),\n  (338, 2),\n  (351, 2),\n  (420, 3),\n  (444, 1),\n  (459, 2),\n  (565, 1),\n  (605, 1),\n  (606, 2),\n  (661, 1),\n  (666, 1),\n  (822, 1),\n  (864, 1),\n  (945, 4),\n  (957, 4),\n  (992, 4),\n  (1008, 1),\n  (1012, 8),\n  (1026, 2),\n  (1111, 1),\n  (1158, 2),\n  (1174, 1),\n  (1195, 2),\n  (1269, 1),\n  (1320, 1),\n  (1326, 1),\n  (1336, 1),\n  (1369, 2),\n  (1407, 4),\n  (1426, 1),\n  (1510, 1),\n  (1521, 1),\n  (1528, 1),\n  (1552, 3),\n  (1563, 4),\n  (1660, 1),\n  (1702, 1),\n  (1728, 1),\n  (1779, 26),\n  (1837, 1),\n  (1931, 2),\n  (1993, 5),\n  (2030, 1),\n  (2105, 4),\n  (2157, 1),\n  (2299, 1),\n  (2304, 1),\n  (2372, 2),\n  (2405, 1),\n  (2412, 7),\n  (2437, 4),\n  (2526, 2),\n  (2595, 2),\n  (2602, 1),\n  (2643, 1),\n  (2675, 1),\n  (2772, 2),\n  (2789, 2),\n  (2909, 1),\n  (2980, 4),\n  (2995, 2),\n  (3035, 1),\n  (3036, 1),\n  (3142, 1),\n  (3147, 4),\n  (3176, 3),\n  (3328, 3),\n  (3331, 1),\n  (3664, 1),\n  (3702, 1),\n  (3731, 2),\n  (3783, 2),\n  (3788, 11),\n  (3930, 1),\n  (3964, 1),\n  (4013, 2),\n  (4023, 1),\n  (4087, 1),\n  (4259, 3),\n  (4372, 3),\n  (4407, 1),\n  (4512, 1),\n  (4566, 1),\n  (4581, 1),\n  (4607, 7),\n  (4697, 2),\n  (4805, 64),\n  (5073, 1),\n  (5177, 2),\n  (5178, 1),\n  (5209, 2),\n  (5984, 1),\n  (6047, 1),\n  (6158, 1),\n  (6199, 1),\n  (6271, 3),\n  (6322, 1),\n  (6513, 4),\n  (6669, 1),\n  (6723, 1),\n  (6773, 7),\n  (6799, 1),\n  (6933, 3),\n  (6942, 1),\n  (7011, 7),\n  (7070, 2),\n  (7077, 1),\n  (7119, 1),\n  (7369, 1),\n  (7491, 2),\n  (7559, 2),\n  (7917, 1),\n  (7996, 5),\n  (8000, 1),\n  (8002, 1),\n  (8223, 2),\n  (8480, 1),\n  (8523, 2),\n  (8551, 1),\n  (8580, 1),\n  (8788, 1),\n  (8828, 1),\n  (9073, 1),\n  (9091, 1),\n  (9264, 1),\n  (9304, 1),\n  (9494, 1),\n  (9785, 1),\n  (9808, 1),\n  (9970, 1),\n  (10144, 1),\n  (10247, 2),\n  (10253, 2),\n  (10285, 1),\n  (10304, 1),\n  (10335, 2),\n  (10517, 4),\n  (10528, 1),\n  (10663, 19),\n  (10672, 6),\n  (10767, 2),\n  (10786, 3),\n  (10878, 2),\n  (11282, 1),\n  (11287, 1),\n  (11359, 1),\n  (11450, 8),\n  (11518, 3),\n  (11978, 1),\n  (11984, 1),\n  (11987, 1),\n  (12000, 4),\n  (12003, 1),\n  (12006, 3),\n  (12008, 4),\n  (12048, 1),\n  (12056, 1),\n  (12057, 1),\n  (12069, 1),\n  (12164, 1),\n  (12183, 1),\n  (12192, 1),\n  (12211, 1),\n  (12213, 1),\n  (12272, 2),\n  (12280, 12),\n  (12289, 1),\n  (12310, 5),\n  (12321, 2),\n  (12346, 1),\n  (12365, 4),\n  (12376, 18),\n  (12409, 2),\n  (12466, 1),\n  (12561, 2),\n  (12668, 1),\n  (12734, 3),\n  (12738, 3),\n  (12855, 1),\n  (12921, 1),\n  (13157, 1),\n  (13193, 1),\n  (13236, 10),\n  (13243, 1),\n  (13293, 3),\n  (13306, 1),\n  (13332, 2),\n  (13342, 1),\n  (13345, 1),\n  (13362, 2),\n  (13372, 4),\n  (13521, 1),\n  (13775, 2),\n  (14219, 1),\n  (14222, 1),\n  (14239, 1),\n  (14491, 1),\n  (14629, 1),\n  (14713, 1),\n  (14752, 8),\n  (14755, 1),\n  (14763, 3),\n  (14771, 1),\n  (15134, 1),\n  (15156, 19),\n  (15310, 2),\n  (15393, 3),\n  (15614, 1),\n  (15801, 2),\n  (15807, 1),\n  (15940, 4),\n  (16030, 1),\n  (16195, 1),\n  (16450, 1),\n  (16516, 1),\n  (16808, 1),\n  (17157, 7),\n  (17481, 1),\n  (17493, 1),\n  (17504, 7),\n  (17509, 1),\n  (17511, 1),\n  (17518, 1),\n  (17519, 1),\n  (18003, 1),\n  (18148, 2),\n  (18599, 1),\n  (18612, 1),\n  (18795, 1),\n  (18810, 4),\n  (18862, 2),\n  (19261, 2),\n  (19350, 1),\n  (20268, 1),\n  (20415, 3),\n  (20456, 6),\n  (20726, 1),\n  (20931, 8),\n  (20983, 1),\n  (23076, 1),\n  (23292, 1),\n  (24983, 1),\n  (25509, 1),\n  (26495, 1),\n  (28129, 1),\n  (28193, 1),\n  (28194, 1),\n  (28195, 1),\n  (28196, 1),\n  (28197, 3),\n  (28198, 1),\n  (28199, 1),\n  (28200, 1),\n  (28201, 1),\n  (28202, 1),\n  (28203, 1),\n  (28204, 1),\n  (28205, 1),\n  (28206, 1),\n  (28207, 1),\n  (28208, 1),\n  (28209, 1),\n  (28210, 1),\n  (28211, 1),\n  (28212, 1),\n  (28213, 1),\n  (28214, 1),\n  (28215, 1),\n  (28216, 1),\n  (28217, 2),\n  (28218, 1),\n  (28219, 1),\n  (28220, 1),\n  (28221, 1),\n  (28222, 1),\n  (28223, 1),\n  (28224, 1),\n  (28225, 1),\n  (28226, 1),\n  (28227, 1),\n  (28228, 2),\n  (28229, 2),\n  (28230, 1),\n  (28231, 1),\n  (28232, 1),\n  (28233, 1),\n  (28234, 1),\n  (28235, 1),\n  (28236, 1),\n  (28237, 1),\n  (28238, 3),\n  (28239, 1)],\n [(7, 12),\n  (195, 1),\n  (307, 1),\n  (397, 1),\n  (444, 1),\n  (531, 1),\n  (583, 1),\n  (630, 3),\n  (640, 8),\n  (725, 1),\n  (908, 1),\n  (1052, 1),\n  (1182, 1),\n  (1336, 1),\n  (1355, 3),\n  (1470, 1),\n  (1682, 2),\n  (1685, 3),\n  (1750, 1),\n  (1760, 1),\n  (1831, 4),\n  (1915, 1),\n  (1939, 2),\n  (1943, 1),\n  (1966, 1),\n  (1980, 1),\n  (2012, 1),\n  (2023, 1),\n  (2053, 1),\n  (2074, 1),\n  (2152, 4),\n  (2226, 4),\n  (2245, 1),\n  (2281, 1),\n  (2355, 1),\n  (2400, 1),\n  (2427, 1),\n  (2444, 1),\n  (3085, 1),\n  (3323, 1),\n  (3335, 1),\n  (3678, 1),\n  (3864, 2),\n  (4261, 2),\n  (4621, 1),\n  (4648, 2),\n  (4850, 1),\n  (5855, 3),\n  (5883, 1),\n  (5895, 1),\n  (5971, 2),\n  (6012, 1),\n  (6222, 1),\n  (6688, 1),\n  (6707, 1),\n  (6885, 1),\n  (6981, 1),\n  (6983, 1),\n  (7244, 1),\n  (7268, 1),\n  (7350, 4),\n  (7606, 1),\n  (7862, 1),\n  (7886, 1),\n  (8542, 2),\n  (8701, 1),\n  (8744, 1),\n  (8925, 1),\n  (8944, 2),\n  (9026, 1),\n  (9215, 1),\n  (9738, 1),\n  (10055, 1),\n  (10345, 1),\n  (10524, 1),\n  (11265, 1),\n  (11497, 2),\n  (11989, 1),\n  (12068, 1),\n  (12086, 1),\n  (12144, 1),\n  (12177, 2),\n  (12270, 1),\n  (12523, 1),\n  (12566, 2),\n  (12572, 1),\n  (12596, 1),\n  (12773, 2),\n  (12883, 1),\n  (13001, 1),\n  (13060, 1),\n  (13083, 1),\n  (13137, 1),\n  (13223, 1),\n  (13236, 1),\n  (13241, 1),\n  (13263, 5),\n  (13470, 1),\n  (13538, 3),\n  (13546, 2),\n  (13565, 1),\n  (13570, 1),\n  (13584, 10),\n  (13593, 2),\n  (13595, 8),\n  (13606, 5),\n  (13626, 1),\n  (13694, 1),\n  (13699, 3),\n  (13809, 1),\n  (13991, 1),\n  (14210, 2),\n  (14311, 1),\n  (14537, 1),\n  (14558, 1),\n  (14671, 1),\n  (14730, 1),\n  (14808, 1),\n  (14971, 4),\n  (15140, 1),\n  (15231, 2),\n  (15307, 1),\n  (15336, 4),\n  (16305, 1),\n  (16917, 1),\n  (17202, 1),\n  (17217, 1),\n  (17388, 1),\n  (17526, 1),\n  (17588, 1),\n  (17603, 1),\n  (18592, 6),\n  (18923, 1),\n  (19013, 1),\n  (19453, 2),\n  (20318, 1),\n  (20656, 1),\n  (21110, 1),\n  (21139, 1),\n  (21588, 1),\n  (22040, 5),\n  (22145, 1),\n  (22337, 5),\n  (22647, 1),\n  (22967, 1),\n  (23083, 1),\n  (23275, 1),\n  (23335, 2),\n  (23936, 1),\n  (24506, 1),\n  (25380, 1),\n  (25733, 1),\n  (27513, 1),\n  (28240, 1),\n  (28241, 1),\n  (28242, 1),\n  (28243, 1),\n  (28244, 1),\n  (28245, 1),\n  (28246, 1),\n  (28247, 1),\n  (28248, 1),\n  (28249, 1),\n  (28250, 1),\n  (28251, 1),\n  (28252, 1),\n  (28253, 1),\n  (28254, 1),\n  (28255, 1),\n  (28256, 1),\n  (28257, 1),\n  (28258, 1),\n  (28259, 1),\n  (28260, 1),\n  (28261, 1),\n  (28262, 1),\n  (28263, 1),\n  (28264, 1),\n  (28265, 1),\n  (28266, 1),\n  (28267, 1),\n  (28268, 1),\n  (28269, 1),\n  (28270, 1),\n  (28271, 1),\n  (28272, 1),\n  (28273, 1),\n  (28274, 1),\n  (28275, 1),\n  (28276, 1),\n  (28277, 1),\n  (28278, 1),\n  (28279, 1),\n  (28280, 1),\n  (28281, 1),\n  (28282, 1),\n  (28283, 1),\n  (28284, 1),\n  (28285, 1),\n  (28286, 1),\n  (28287, 1),\n  (28288, 1),\n  (28289, 1),\n  (28290, 1),\n  (28291, 1),\n  (28292, 1),\n  (28293, 1),\n  (28294, 1),\n  (28295, 1),\n  (28296, 1),\n  (28297, 1),\n  (28298, 1),\n  (28299, 1),\n  (28300, 1),\n  (28301, 1),\n  (28302, 1),\n  (28303, 1),\n  (28304, 1),\n  (28305, 4),\n  (28306, 1),\n  (28307, 7),\n  (28308, 3),\n  (28309, 1),\n  (28310, 1),\n  (28311, 2),\n  (28312, 1),\n  (28313, 1),\n  (28314, 1),\n  (28315, 1),\n  (28316, 1),\n  (28317, 1),\n  (28318, 1),\n  (28319, 1),\n  (28320, 1),\n  (28321, 1),\n  (28322, 1)],\n [(16, 1),\n  (325, 1),\n  (332, 1),\n  (337, 2),\n  (365, 1),\n  (417, 1),\n  (527, 1),\n  (531, 1),\n  (781, 1),\n  (921, 2),\n  (1012, 1),\n  (1327, 1),\n  (1399, 3),\n  (1455, 2),\n  (1608, 2),\n  (1675, 3),\n  (1794, 1),\n  (2031, 1),\n  (2150, 4),\n  (2248, 3),\n  (2711, 1),\n  (2730, 2),\n  (2831, 1),\n  (2984, 1),\n  (3355, 1),\n  (3498, 1),\n  (3528, 1),\n  (3623, 1),\n  (3965, 1),\n  (4866, 1),\n  (5128, 1),\n  (5575, 1),\n  (6011, 1),\n  (6092, 1),\n  (6101, 2),\n  (6284, 1),\n  (7970, 2),\n  (8033, 1),\n  (8076, 1),\n  (8617, 1),\n  (8849, 2),\n  (8897, 1),\n  (8914, 1),\n  (9521, 1),\n  (10055, 1),\n  (10192, 1),\n  (10336, 1),\n  (10450, 2),\n  (10517, 1),\n  (11670, 1),\n  (11879, 1),\n  (11984, 1),\n  (11989, 1),\n  (12234, 1),\n  (12295, 6),\n  (12666, 2),\n  (12667, 1),\n  (12734, 2),\n  (12790, 1),\n  (13083, 2),\n  (13089, 2),\n  (13100, 1),\n  (13104, 2),\n  (13180, 1),\n  (13217, 4),\n  (13772, 1),\n  (14022, 1),\n  (15447, 1),\n  (15987, 1),\n  (15988, 7),\n  (16894, 1),\n  (16900, 2),\n  (16951, 4),\n  (16971, 5),\n  (17525, 1),\n  (17573, 2),\n  (20180, 2),\n  (20350, 1),\n  (20969, 1),\n  (22492, 3),\n  (24947, 2),\n  (25090, 1),\n  (26152, 1),\n  (27748, 1),\n  (28323, 1),\n  (28324, 1),\n  (28325, 1),\n  (28326, 1),\n  (28327, 3),\n  (28328, 1),\n  (28329, 1),\n  (28330, 1)],\n [(1516, 1),\n  (1674, 1),\n  (2759, 1),\n  (3683, 1),\n  (4278, 1),\n  (4877, 4),\n  (6101, 2),\n  (6762, 1),\n  (8047, 1),\n  (9171, 1),\n  (9703, 1),\n  (12057, 1),\n  (12144, 1),\n  (12164, 2),\n  (12457, 1),\n  (12668, 2),\n  (12731, 1),\n  (13083, 3),\n  (13095, 1),\n  (13096, 1),\n  (13097, 1),\n  (13099, 1),\n  (13100, 3),\n  (13119, 1),\n  (13200, 1),\n  (13205, 1),\n  (13215, 2),\n  (13379, 2),\n  (13380, 1),\n  (13904, 1),\n  (13951, 1),\n  (14032, 1),\n  (14034, 2),\n  (14035, 1),\n  (14048, 1),\n  (14100, 1),\n  (15179, 1),\n  (15465, 1),\n  (15644, 1),\n  (16469, 1),\n  (17124, 1),\n  (18351, 1),\n  (19629, 1),\n  (25407, 1),\n  (25426, 1),\n  (28331, 1),\n  (28332, 1)],\n ...]"
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'0.006*\"高考\" + 0.005*\"说\" + 0.004*\"万\" + 0.004*\"中\" + 0.003*\"离婚\"'"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "# 1号的分类结果展示\n",
    "lda.print_topic(0, topn=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(0, '0.006*\"高考\" + 0.005*\"说\" + 0.004*\"万\" + 0.004*\"中\" + 0.003*\"离婚\"')\n(1, '0.006*\"女人\" + 0.006*\"剧组\" + 0.006*\"中\" + 0.006*\"男人\" + 0.004*\"中国\"')\n(2, '0.007*\"志愿\" + 0.005*\"纹身\" + 0.004*\"相亲\" + 0.004*\"防晒\" + 0.003*\"中\"')\n(3, '0.009*\"男人\" + 0.007*\"女人\" + 0.007*\"该剧\" + 0.007*\"中\" + 0.006*\"粉丝\"')\n(4, '0.005*\"节目\" + 0.005*\"汤姆\" + 0.004*\"阿迪达斯\" + 0.004*\"郭德纲\" + 0.003*\"播出\"')\n(5, '0.007*\"Ｍ\" + 0.007*\"中\" + 0.006*\"Ｓ\" + 0.005*\"皮肤\" + 0.005*\"Ｉ\"')\n(6, '0.007*\"Ｍ\" + 0.007*\"万\" + 0.005*\"电视剧\" + 0.004*\"Ｖ\" + 0.004*\"号\"')\n(7, '0.014*\"导演\" + 0.011*\"电影\" + 0.007*\"影片\" + 0.006*\"中\" + 0.005*\"主演\"')\n(8, '0.004*\"Ｎ\" + 0.003*\"中\" + 0.002*\"Ｍ\" + 0.002*\"於\" + 0.002*\"砉\"')\n(9, '0.008*\"中\" + 0.005*\"学生\" + 0.004*\"中国\" + 0.004*\"欧洲杯\" + 0.002*\"文化\"')\n(10, '0.005*\"Ｔ\" + 0.004*\"中\" + 0.003*\"ａ\" + 0.003*\"学校\" + 0.003*\"Ｌ\"')\n(11, '0.006*\"中\" + 0.005*\"张绍\" + 0.004*\"中国\" + 0.003*\"女性\" + 0.003*\"男人\"')\n(12, '0.012*\"中国\" + 0.007*\"观众\" + 0.005*\"中\" + 0.005*\"说\" + 0.004*\"美国\"')\n(13, '0.010*\"恋情\" + 0.010*\"分手\" + 0.006*\"中\" + 0.004*\"小说\" + 0.004*\"化妆\"')\n(14, '0.018*\"ａ\" + 0.017*\"ｅ\" + 0.013*\"ｏ\" + 0.012*\"ｉ\" + 0.011*\"ｒ\"')\n(15, '0.006*\"说\" + 0.006*\"中\" + 0.004*\"中国\" + 0.003*\"选手\" + 0.003*\"Ｓ\"')\n(16, '0.008*\"肌肤\" + 0.007*\"中\" + 0.005*\"吃\" + 0.005*\"食物\" + 0.005*\"性爱\"')\n(17, '0.014*\"说\" + 0.009*\"中\" + 0.007*\"饰演\" + 0.007*\"男人\" + 0.006*\"爱\"')\n(18, '0.016*\"官兵\" + 0.012*\"爆料\" + 0.010*\"部队\" + 0.007*\"武警\" + 0.004*\"工作\"')\n(19, '0.009*\"节目\" + 0.008*\"卫视\" + 0.008*\"中\" + 0.007*\"邱\" + 0.006*\"主持人\"')\n"
    }
   ],
   "source": [
    "# 20个分类结果\n",
    "for topic in lda.print_topics(num_topics=20, num_words=5):\n",
    "    print(topic)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 贝叶斯分类\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                         contents_clean label\n4995  [天气, 炎热, 补水, 变得, 美国, 跑步, 世界, 杂志, 报道, 喝水, 身体, 补...    时尚\n4996  [不想, 说, 话, 刺激, 说, 做, 只能, 走, 离开, 伤心地, 想起, 一句, 话...    时尚\n4997  [岁, 刘晓庆, 最新, 嫩照, Ｏ, 衷, 诘, 牧跸, 庆, 看不出, 岁, 秒杀, 刘...    时尚\n4998  [导语, 做, 爸爸, 一种, 幸福, 无论是, 领养, 亲生, 更何况, 影视剧, 中, ...    时尚\n4999  [全球, 最美, 女人, 合成图, 国, 整形外科, 教授, 李承哲, 国际, 学术, 杂志...    时尚",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>contents_clean</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4995</th>\n      <td>[天气, 炎热, 补水, 变得, 美国, 跑步, 世界, 杂志, 报道, 喝水, 身体, 补...</td>\n      <td>时尚</td>\n    </tr>\n    <tr>\n      <th>4996</th>\n      <td>[不想, 说, 话, 刺激, 说, 做, 只能, 走, 离开, 伤心地, 想起, 一句, 话...</td>\n      <td>时尚</td>\n    </tr>\n    <tr>\n      <th>4997</th>\n      <td>[岁, 刘晓庆, 最新, 嫩照, Ｏ, 衷, 诘, 牧跸, 庆, 看不出, 岁, 秒杀, 刘...</td>\n      <td>时尚</td>\n    </tr>\n    <tr>\n      <th>4998</th>\n      <td>[导语, 做, 爸爸, 一种, 幸福, 无论是, 领养, 亲生, 更何况, 影视剧, 中, ...</td>\n      <td>时尚</td>\n    </tr>\n    <tr>\n      <th>4999</th>\n      <td>[全球, 最美, 女人, 合成图, 国, 整形外科, 教授, 李承哲, 国际, 学术, 杂志...</td>\n      <td>时尚</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "df_train = pd.DataFrame({'contents_clean': contents_clean, 'label': df_news['category']})\n",
    "df_train.tail()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array(['汽车', '财经', '科技', '健康', '体育', '教育', '文化', '军事', '娱乐', '时尚'],\n      dtype=object)"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "df_train.label.unique()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                      contents_clean  label\n0  [经销商, 电话, 试驾, 订车, Ｕ, 憬, 杭州, 滨江区, 江陵, 路, 号, 转, ...      1\n1  [呼叫, 热线, 服务, 邮箱, ｋ, ｆ, ｐ, ｅ, ｏ, ｐ, ｌ, ｅ, ｄ, ａ,...      1\n2  [Ｍ, Ｉ, Ｎ, Ｉ, 品牌, 二月, 公布, 最新, Ｍ, Ｉ, Ｎ, Ｉ, 新, 概念...      1\n3  [清仓, 甩卖, 一汽, 夏利, Ｎ, 威志, Ｖ, 低至, 万, 启新, 中国, 一汽, ...      1\n4  [日内瓦, 车展, 见到, 高尔夫, 家族, 新, 成员, 高尔夫, 敞篷版, 款, 全新,...      1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>contents_clean</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[经销商, 电话, 试驾, 订车, Ｕ, 憬, 杭州, 滨江区, 江陵, 路, 号, 转, ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[呼叫, 热线, 服务, 邮箱, ｋ, ｆ, ｐ, ｅ, ｏ, ｐ, ｌ, ｅ, ｄ, ａ,...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[Ｍ, Ｉ, Ｎ, Ｉ, 品牌, 二月, 公布, 最新, Ｍ, Ｉ, Ｎ, Ｉ, 新, 概念...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[清仓, 甩卖, 一汽, 夏利, Ｎ, 威志, Ｖ, 低至, 万, 启新, 中国, 一汽, ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[日内瓦, 车展, 见到, 高尔夫, 家族, 新, 成员, 高尔夫, 敞篷版, 款, 全新,...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "label_mapping = {\"汽车\": 1, \"财经\": 2, \"科技\": 3, \"健康\": 4, \"体育\":5, \"教育\": 6,\"文化\": 7,\"军事\": 8,\"娱乐\": 9,\"时尚\": 0}\n",
    "df_train['label'] = df_train['label'].map(label_mapping)\n",
    "df_train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "x_train, x_test, y_train, y_test = train_test_split(df_train['contents_clean'].values, df_train['label'].values, random_state=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'上海'"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "x_train[0][1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'中新网 上海 日电 于俊 父亲节 网络 吃 一顿 电影 快餐 微 电影 爸 对不起 我爱你 定于 本月 父亲节 当天 各大 视频 网站 首映 葜 谱 鞣 剑 保慈 障蚣 钦 呓 樯 埽 ⒌ 缬 埃 ǎ 停 椋 悖 颍 铩 妫 椋 恚 称 微型 电影 新 媒体 平台 播放 状态 短时 休闲 状态 观看 完整 策划 系统 制作 体系 支持 显示 较完整 故事情节 电影 微 超短 放映 微 周期 制作 天 数周 微 规模 投资 人民币 几千 数万元 每部 内容 融合 幽默 搞怪 时尚 潮流 人文 言情 公益 教育 商业 定制 主题 单独 成篇 系列 成剧 唇 开播 微 电影 爸 对不起 我爱你 讲述 一对 父子 观念 缺少 沟通 导致 关系 父亲 传统 固执 钟情 传统 生活 方式 儿子 新派 音乐 达 习惯 晚出 早 生活 性格 张扬 叛逆 两种 截然不同 生活 方式 理念 差异 一场 父子 间 拉开序幕 子 失手 打破 父亲 心爱 物品 父亲 赶出 家门 剧情 演绎 父亲节 妹妹 哥哥 化解 父亲 这场 矛盾 映逋坏 嚼 斫 狻 ⒍ 粤 ⒌ 桨容 争执 退让 传统 尴尬 父子 尴尬 情 男人 表达 心中 那份 感恩 一杯 滤挂 咖啡 父亲节 变得 温馨 镁 缬 缮 虾 Ｎ 逄 煳 幕 传播 迪欧 咖啡 联合 出品 出品人 希望 观摩 扪心自问 父亲节 父亲 记得 父亲 生日 哪一天 父亲 爱喝 跨出 家门 那一刻 感觉 一颗 颤动 心 操劳 天下 儿女 父亲节 大声 喊出 父亲 家人 爱 完'"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "words = []\n",
    "for line_index in range(len(x_train)):\n",
    "    try:\n",
    "        #x_train[line_index][word_index] = str(x_train[line_index][word_index])\n",
    "        words.append(' '.join(x_train[line_index]))\n",
    "    except:\n",
    "        print (line_index,word_index)\n",
    "words[0]        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "3750\n"
    }
   ],
   "source": [
    "print(len(words))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['bird', 'cat', 'dog', 'fish']\n[[0 1 1 1]\n [0 2 1 0]\n [1 0 0 1]\n [1 0 0 0]]\n[2 3 2 2]\n  (0, 2)\t1\n  (0, 1)\t1\n  (0, 3)\t1\n  (1, 2)\t1\n  (1, 1)\t2\n  (2, 3)\t1\n  (2, 0)\t1\n  (3, 0)\t1\n"
    }
   ],
   "source": [
    "# 特征选择\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 不能是list-of-list格式,要list of str\n",
    "texts=[\"dog cat fish\",\"dog cat cat\",\"fish bird\", 'bird']\n",
    "cv = CountVectorizer()\n",
    "cv_fit = cv.fit_transform(texts)\n",
    "print(cv.get_feature_names())\n",
    "print(cv_fit.toarray())\n",
    "print(cv_fit.toarray().sum(axis=0))\n",
    "print(cv_fit)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['bird', 'cat', 'cat cat', 'cat fish', 'dog', 'dog cat', 'dog cat cat', 'dog cat fish', 'fish', 'fish bird']\n[[0 1 0 1 1 1 0 1 1 0]\n [0 2 1 0 1 1 1 0 0 0]\n [1 0 0 0 0 0 0 0 1 1]\n [1 0 0 0 0 0 0 0 0 0]]\n[2 3 1 1 2 2 1 1 2 1]\n"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "texts=[\"dog cat fish\",\"dog cat cat\",\"fish bird\", 'bird']\n",
    "cv = CountVectorizer(ngram_range=(1,4))\n",
    "cv_fit=cv.fit_transform(texts)\n",
    "\n",
    "print(cv.get_feature_names())\n",
    "print(cv_fit.toarray())\n",
    "\n",
    "\n",
    "print(cv_fit.toarray().sum(axis=0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "CountVectorizer(lowercase=False, max_features=4000)"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(analyzer='word', max_features=4000,  lowercase = False)\n",
    "vec.fit(words)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "MultinomialNB()"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(vec.transform(words), y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'国家 公务员 考试 申论 应用文 类 试题 实质 一道 集 概括 分析 提出 解决问题 一体 综合性 试题 说 一道 客观 凝练 申发 论述 文章 题目 分析 历年 国考 申论 真题 公文 类 试题 类型 多样 包括 公文 类 事务性 文书 类 题材 从题 干 作答 材料 内容 整合 分析 无需 太 创造性 发挥 纵观 历年 申论 真题 作答 应用文 类 试题 文种 格式 作出 特别 重在 内容 考查 行文 格式 考生 平常心 面对 应用文 类 试题 准确 把握 作答 领会 内在 含义 把握 题材 主旨 材料 结构 轻松 应对 应用文 类 试题 Ｒ 弧 ⒆ 钒 盐 展文 写作 原则 Ｔ 材料 中来 应用文 类 试题 材料 总体 把握 客观 考生 材料 中来 材料 中 把握 材料 准确 理解 题材 主旨 Ｔ 政府 角度 作答 应用文 类 试题 更应 注重 政府 角度 观点 政府 角度 出发 原则 表述 观点 提出 解决 之策 考生 作答 站 政府 人员 角度 看待 提出 解决问题 Ｔ 文体 结构 形式 考查 重点 文体 结构 大部分 评分 关键点 解答 方法 薄 ⒆ ス 丶 词 明 方向 作答 题目 题干 作答 作答 方向 作答 角度 关键 向导 考生 仔细阅读 题干 作答 抓住 关键词 作答 方向 相关 要点 整理 作答 思路 年国考 地市级 真 题为 例 潦惺姓 府 宣传 推进 近海 水域 污染 整治 工作 请 给定 资料 市政府 工作人员 身份 草拟 一份 宣传 纲要 Ｒ 求 保对 宣传 内容 要点 提纲挈领 陈述 玻 体现 政府 精神 全市 各界 关心 支持 污染 整治 工作 通俗易懂 超过 字 肮 丶 词 近海 水域 污染 整治 工作 市政府 工作人员 身份 宣传 纲要 提纲挈领 陈述 体现 政府 精神 全市 各界 关心 支持 污染 整治 工作 通俗易懂 提示 归结 作答 要点 包括 污染 情况 原因 解决 对策 作答 思路 情况 原因 对策 意义 逻辑 顺序 安排 文章 结构 病 ⒋ 缶殖 龇 ⅲ 明 结构 解答 应用文 类 试题 考生 材料 整体 出发 大局 出发 高屋建瓴 把握 材料 主题 思想 事件 起因 解决 对策 阅读文章 构建 文章 结构 直至 快速 解答 场 ⒗ 硭 乘悸 罚明 逻辑 应用文 类 试题 严密 逻辑思维 情况 原因 对策 意义 考生 作答 先 弄清楚 解答 思路 统筹安排 脉络 清晰 逻辑 表达 内容 表述 础 把握 明 详略 考生 仔细阅读 分析 揣摩 应用文 类 试题 内容 答题 时要 详略 得当 主次 分明 安排 内容 增加 文章 层次感 阅卷 老师 阅卷 时能 明白 清晰 一目了然 玻埃 保蹦旯 考 考试 申论 试卷 分为 省级 地市级 两套 试卷 能力 大有 省级 申论 试题 考生 宏观 角度看 注重 深度 广度 考生 深谋远虑 地市级 试题 考生 微观 视角 观察 侧重 考查 解决 能力 考生 贯彻执行 作答 区别对待'"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "test_words = []\n",
    "for line_index in range(len(x_test)):\n",
    "    try:\n",
    "        #x_train[line_index][word_index] = str(x_train[line_index][word_index])\n",
    "        test_words.append(' '.join(x_test[line_index]))\n",
    "    except:\n",
    "         print (line_index,word_index)\n",
    "test_words[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.8032"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "classifier.score(vec.transform(test_words), y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "TfidfVectorizer(lowercase=False, max_features=4000)"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer='word', max_features=4000,  lowercase = False)\n",
    "vectorizer.fit(words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "MultinomialNB()"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(vectorizer.transform(words), y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.8136"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "classifier.score(vectorizer.transform(test_words), y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                         contents_clean label\n4995  [天气, 炎热, 补水, 变得, 美国, 跑步, 世界, 杂志, 报道, 喝水, 身体, 补...    时尚\n4996  [不想, 说, 话, 刺激, 说, 做, 只能, 走, 离开, 伤心地, 想起, 一句, 话...    时尚\n4997  [岁, 刘晓庆, 最新, 嫩照, Ｏ, 衷, 诘, 牧跸, 庆, 看不出, 岁, 秒杀, 刘...    时尚\n4998  [导语, 做, 爸爸, 一种, 幸福, 无论是, 领养, 亲生, 更何况, 影视剧, 中, ...    时尚\n4999  [全球, 最美, 女人, 合成图, 国, 整形外科, 教授, 李承哲, 国际, 学术, 杂志...    时尚",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>contents_clean</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4995</th>\n      <td>[天气, 炎热, 补水, 变得, 美国, 跑步, 世界, 杂志, 报道, 喝水, 身体, 补...</td>\n      <td>时尚</td>\n    </tr>\n    <tr>\n      <th>4996</th>\n      <td>[不想, 说, 话, 刺激, 说, 做, 只能, 走, 离开, 伤心地, 想起, 一句, 话...</td>\n      <td>时尚</td>\n    </tr>\n    <tr>\n      <th>4997</th>\n      <td>[岁, 刘晓庆, 最新, 嫩照, Ｏ, 衷, 诘, 牧跸, 庆, 看不出, 岁, 秒杀, 刘...</td>\n      <td>时尚</td>\n    </tr>\n    <tr>\n      <th>4998</th>\n      <td>[导语, 做, 爸爸, 一种, 幸福, 无论是, 领养, 亲生, 更何况, 影视剧, 中, ...</td>\n      <td>时尚</td>\n    </tr>\n    <tr>\n      <th>4999</th>\n      <td>[全球, 最美, 女人, 合成图, 国, 整形外科, 教授, 李承哲, 国际, 学术, 杂志...</td>\n      <td>时尚</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "df_train = pd.DataFrame({'contents_clean': contents_clean, 'label': df_news['category']})\n",
    "df_train.tail()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array(['汽车', '财经', '科技', '健康', '体育', '教育', '文化', '军事', '娱乐', '时尚'],\n      dtype=object)"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "df_train.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                      contents_clean  label\n0  [经销商, 电话, 试驾, 订车, Ｕ, 憬, 杭州, 滨江区, 江陵, 路, 号, 转, ...      1\n1  [呼叫, 热线, 服务, 邮箱, ｋ, ｆ, ｐ, ｅ, ｏ, ｐ, ｌ, ｅ, ｄ, ａ,...      1\n2  [Ｍ, Ｉ, Ｎ, Ｉ, 品牌, 二月, 公布, 最新, Ｍ, Ｉ, Ｎ, Ｉ, 新, 概念...      1\n3  [清仓, 甩卖, 一汽, 夏利, Ｎ, 威志, Ｖ, 低至, 万, 启新, 中国, 一汽, ...      1\n4  [日内瓦, 车展, 见到, 高尔夫, 家族, 新, 成员, 高尔夫, 敞篷版, 款, 全新,...      1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>contents_clean</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[经销商, 电话, 试驾, 订车, Ｕ, 憬, 杭州, 滨江区, 江陵, 路, 号, 转, ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[呼叫, 热线, 服务, 邮箱, ｋ, ｆ, ｐ, ｅ, ｏ, ｐ, ｌ, ｅ, ｄ, ａ,...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[Ｍ, Ｉ, Ｎ, Ｉ, 品牌, 二月, 公布, 最新, Ｍ, Ｉ, Ｎ, Ｉ, 新, 概念...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[清仓, 甩卖, 一汽, 夏利, Ｎ, 威志, Ｖ, 低至, 万, 启新, 中国, 一汽, ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[日内瓦, 车展, 见到, 高尔夫, 家族, 新, 成员, 高尔夫, 敞篷版, 款, 全新,...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "label_mapping = {\"汽车\": 1, \"财经\": 2, \"科技\": 3, \"健康\": 4, \"体育\":5, \"教育\": 6,\"文化\": 7,\"军事\": 8,\"娱乐\": 9,\"时尚\": 0}\n",
    "df_train['label'] = df_train['label'].map(label_mapping)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "x_train, x_test, y_train, y_test = train_test_split(df_train['contents_clean'].values, df_train['label'].values, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'上海'"
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "x_train[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'中新网 上海 日电 于俊 父亲节 网络 吃 一顿 电影 快餐 微 电影 爸 对不起 我爱你 定于 本月 父亲节 当天 各大 视频 网站 首映 葜 谱 鞣 剑 保慈 障蚣 钦 呓 樯 埽 ⒌ 缬 埃 ǎ 停 椋 悖 颍 铩 妫 椋 恚 称 微型 电影 新 媒体 平台 播放 状态 短时 休闲 状态 观看 完整 策划 系统 制作 体系 支持 显示 较完整 故事情节 电影 微 超短 放映 微 周期 制作 天 数周 微 规模 投资 人民币 几千 数万元 每部 内容 融合 幽默 搞怪 时尚 潮流 人文 言情 公益 教育 商业 定制 主题 单独 成篇 系列 成剧 唇 开播 微 电影 爸 对不起 我爱你 讲述 一对 父子 观念 缺少 沟通 导致 关系 父亲 传统 固执 钟情 传统 生活 方式 儿子 新派 音乐 达 习惯 晚出 早 生活 性格 张扬 叛逆 两种 截然不同 生活 方式 理念 差异 一场 父子 间 拉开序幕 子 失手 打破 父亲 心爱 物品 父亲 赶出 家门 剧情 演绎 父亲节 妹妹 哥哥 化解 父亲 这场 矛盾 映逋坏 嚼 斫 狻 ⒍ 粤 ⒌ 桨容 争执 退让 传统 尴尬 父子 尴尬 情 男人 表达 心中 那份 感恩 一杯 滤挂 咖啡 父亲节 变得 温馨 镁 缬 缮 虾 Ｎ 逄 煳 幕 传播 迪欧 咖啡 联合 出品 出品人 希望 观摩 扪心自问 父亲节 父亲 记得 父亲 生日 哪一天 父亲 爱喝 跨出 家门 那一刻 感觉 一颗 颤动 心 操劳 天下 儿女 父亲节 大声 喊出 父亲 家人 爱 完'"
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "words = []\n",
    "for line_index in range(len(x_train)):\n",
    "    try:\n",
    "        #x_train[line_index][word_index] = str(x_train[line_index][word_index])\n",
    "        words.append(' '.join(x_train[line_index]))\n",
    "    except:\n",
    "        print (line_index,word_index)\n",
    "words[0]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "3750\n"
    }
   ],
   "source": [
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['bird', 'cat', 'dog', 'fish']\n[[0 1 1 1]\n [0 2 1 0]\n [1 0 0 1]\n [1 0 0 0]]\n[2 3 2 2]\n  (0, 2)\t1\n  (0, 1)\t1\n  (0, 3)\t1\n  (1, 2)\t1\n  (1, 1)\t2\n  (2, 3)\t1\n  (2, 0)\t1\n  (3, 0)\t1\n"
    }
   ],
   "source": [
    "# 特征选择\n",
    "# 例子\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 不能是list-of-list格式,要list of str\n",
    "texts=[\"dog cat fish\",\"dog cat cat\",\"fish bird\", 'bird']\n",
    "cv = CountVectorizer()\n",
    "cv_fit = cv.fit_transform(texts)\n",
    "print(cv.get_feature_names())\n",
    "print(cv_fit.toarray())\n",
    "print(cv_fit.toarray().sum(axis=0))\n",
    "print(cv_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['bird', 'cat', 'cat cat', 'cat fish', 'dog', 'dog cat', 'dog cat cat', 'dog cat fish', 'fish', 'fish bird']\n[[0 1 0 1 1 1 0 1 1 0]\n [0 2 1 0 1 1 1 0 0 0]\n [1 0 0 0 0 0 0 0 1 1]\n [1 0 0 0 0 0 0 0 0 0]]\n[2 3 1 1 2 2 1 1 2 1]\n"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "texts=[\"dog cat fish\",\"dog cat cat\",\"fish bird\", 'bird']\n",
    "cv = CountVectorizer(ngram_range=(1,4)) # 取词长度为1到3,,维度\n",
    "cv_fit=cv.fit_transform(texts)\n",
    "\n",
    "print(cv.get_feature_names())\n",
    "print(cv_fit.toarray())\n",
    "\n",
    "\n",
    "print(cv_fit.toarray().sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "CountVectorizer(lowercase=False, max_features=4000)"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(analyzer='word', max_features=4000,  lowercase = False)\n",
    "vec.fit(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "CountVectorizer(lowercase=False, max_features=4000)"
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "MultinomialNB()"
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "# MultinomialNB 朴素贝叶斯分类器\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(vec.transform(words), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'国家 公务员 考试 申论 应用文 类 试题 实质 一道 集 概括 分析 提出 解决问题 一体 综合性 试题 说 一道 客观 凝练 申发 论述 文章 题目 分析 历年 国考 申论 真题 公文 类 试题 类型 多样 包括 公文 类 事务性 文书 类 题材 从题 干 作答 材料 内容 整合 分析 无需 太 创造性 发挥 纵观 历年 申论 真题 作答 应用文 类 试题 文种 格式 作出 特别 重在 内容 考查 行文 格式 考生 平常心 面对 应用文 类 试题 准确 把握 作答 领会 内在 含义 把握 题材 主旨 材料 结构 轻松 应对 应用文 类 试题 Ｒ 弧 ⒆ 钒 盐 展文 写作 原则 Ｔ 材料 中来 应用文 类 试题 材料 总体 把握 客观 考生 材料 中来 材料 中 把握 材料 准确 理解 题材 主旨 Ｔ 政府 角度 作答 应用文 类 试题 更应 注重 政府 角度 观点 政府 角度 出发 原则 表述 观点 提出 解决 之策 考生 作答 站 政府 人员 角度 看待 提出 解决问题 Ｔ 文体 结构 形式 考查 重点 文体 结构 大部分 评分 关键点 解答 方法 薄 ⒆ ス 丶 词 明 方向 作答 题目 题干 作答 作答 方向 作答 角度 关键 向导 考生 仔细阅读 题干 作答 抓住 关键词 作答 方向 相关 要点 整理 作答 思路 年国考 地市级 真 题为 例 潦惺姓 府 宣传 推进 近海 水域 污染 整治 工作 请 给定 资料 市政府 工作人员 身份 草拟 一份 宣传 纲要 Ｒ 求 保对 宣传 内容 要点 提纲挈领 陈述 玻 体现 政府 精神 全市 各界 关心 支持 污染 整治 工作 通俗易懂 超过 字 肮 丶 词 近海 水域 污染 整治 工作 市政府 工作人员 身份 宣传 纲要 提纲挈领 陈述 体现 政府 精神 全市 各界 关心 支持 污染 整治 工作 通俗易懂 提示 归结 作答 要点 包括 污染 情况 原因 解决 对策 作答 思路 情况 原因 对策 意义 逻辑 顺序 安排 文章 结构 病 ⒋ 缶殖 龇 ⅲ 明 结构 解答 应用文 类 试题 考生 材料 整体 出发 大局 出发 高屋建瓴 把握 材料 主题 思想 事件 起因 解决 对策 阅读文章 构建 文章 结构 直至 快速 解答 场 ⒗ 硭 乘悸 罚明 逻辑 应用文 类 试题 严密 逻辑思维 情况 原因 对策 意义 考生 作答 先 弄清楚 解答 思路 统筹安排 脉络 清晰 逻辑 表达 内容 表述 础 把握 明 详略 考生 仔细阅读 分析 揣摩 应用文 类 试题 内容 答题 时要 详略 得当 主次 分明 安排 内容 增加 文章 层次感 阅卷 老师 阅卷 时能 明白 清晰 一目了然 玻埃 保蹦旯 考 考试 申论 试卷 分为 省级 地市级 两套 试卷 能力 大有 省级 申论 试题 考生 宏观 角度看 注重 深度 广度 考生 深谋远虑 地市级 试题 考生 微观 视角 观察 侧重 考查 解决 能力 考生 贯彻执行 作答 区别对待'"
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "test_words = []\n",
    "for line_index in range(len(x_test)):\n",
    "    try:\n",
    "        #x_train[line_index][word_index] = str(x_train[line_index][word_index])\n",
    "        test_words.append(' '.join(x_test[line_index]))\n",
    "    except:\n",
    "         print (line_index,word_index)\n",
    "test_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.8032"
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "classifier.score(vec.transform(test_words), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "TfidfVectorizer(lowercase=False, max_features=4000)"
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "# 将原始文档集合转换为TF-IDF特征矩阵。相当于CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer='word', max_features=4000,  lowercase = False)\n",
    "vectorizer.fit(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "MultinomialNB()"
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(vectorizer.transform(words), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "TfidfVectorizer(lowercase=False, max_features=4000)"
     },
     "metadata": {},
     "execution_count": 87
    }
   ],
   "source": [
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.8136"
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "classifier.score(vectorizer.transform(test_words), y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}